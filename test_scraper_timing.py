"""
Script de verificaci√≥n de tiempos de ScraperRalf.

Este script prueba que el sistema espera correctamente a que todas las fuentes
de ScraperRalf completen su b√∫squeda antes de procesar los resultados.

Ejecutar: python test_scraper_timing.py
"""

import time
import requests
import json
from datetime import datetime

# Configuraci√≥n
SCRAPER_URL = "http://127.0.0.1:5000/api/search"
TEST_QUERY = "inteligencia artificial"

print("=" * 80)
print("üß™ TEST DE INTEGRACI√ìN CON SCRAPERRALF")
print("=" * 80)
print()

# Verificar que ScraperRalf est√© corriendo
print("1Ô∏è‚É£ Verificando que ScraperRalf est√© disponible...")
try:
    health_check = requests.get("http://127.0.0.1:5000/", timeout=5)
    if health_check.status_code == 200:
        print("   ‚úÖ ScraperRalf est√° corriendo en http://127.0.0.1:5000")
    else:
        print(f"   ‚ö†Ô∏è ScraperRalf responde con c√≥digo {health_check.status_code}")
except Exception as e:
    print(f"   ‚ùå ERROR: ScraperRalf NO est√° disponible: {e}")
    print("   üí° Soluci√≥n: Ejecuta 'python app.py' en el directorio de ScraperRalf")
    exit(1)

print()
print("2Ô∏è‚É£ Ejecutando b√∫squeda con timeout de 90 segundos...")
print(f"   Query: '{TEST_QUERY}'")
print(f"   Max results: 5 por fuente")
print()

# Medir tiempo de ejecuci√≥n
start_time = time.time()
start_datetime = datetime.now()

print(f"   ‚è±Ô∏è Inicio: {start_datetime.strftime('%H:%M:%S')}")
print("   ‚è≥ Esperando respuesta (esto puede tardar hasta 90 segundos)...")
print()

try:
    response = requests.get(
        SCRAPER_URL,
        params={"q": TEST_QUERY, "max_results": 5},
        timeout=90,  # Mismo timeout que en tools.py
        headers={"User-Agent": "MultiAgent-NewsSystem-Test/1.0"},
    )

    end_time = time.time()
    elapsed_time = end_time - start_time

    print(f"   ‚úÖ Respuesta recibida en {elapsed_time:.2f} segundos")
    print()

    # Parsear respuesta
    data = response.json()

    # An√°lisis de resultados
    print("3Ô∏è‚É£ An√°lisis de Resultados:")
    print("   " + "-" * 76)

    if "results" in data:
        results = data["results"]
        total_results = len(results)

        print(f"   Total de art√≠culos: {total_results}")
        print()

        # Clasificar por fuente
        sources = {}
        for result in results:
            source = result.get("source", "Desconocido")
            if source not in sources:
                sources[source] = []
            sources[source].append(result)

        print("   üìä Distribuci√≥n por fuente:")
        print()

        # Tier 1: Fuentes locales
        tier1_sources = ["La Rep√∫blica", "El Comercio", "Infobae"]
        tier1_count = 0
        tier1_total_chars = 0

        print("   üü¢ TIER 1 - Fuentes Locales (Contenido Completo):")
        for source_name in tier1_sources:
            if source_name in sources:
                articles = sources[source_name]
                tier1_count += len(articles)

                for article in articles:
                    content_length = len(article.get("content", ""))
                    tier1_total_chars += content_length

                print(f"      ‚Ä¢ {source_name}: {len(articles)} art√≠culo(s)")
                print(
                    f"        Longitud promedio: {content_length / len(articles):.0f} caracteres"
                )
                print(f"        M√©todo: {article.get('method', 'N/A')}")

        print()

        # Tier 2: APIs globales
        tier2_sources = ["NewsAPI", "TheNewsAPI"]
        tier2_count = 0
        tier2_total_chars = 0

        print("   üü° TIER 2 - APIs Globales (Snippets):")
        for source_name in tier2_sources:
            if source_name in sources:
                articles = sources[source_name]
                tier2_count += len(articles)

                for article in articles:
                    content_length = len(article.get("content", ""))
                    tier2_total_chars += content_length

                print(f"      ‚Ä¢ {source_name}: {len(articles)} art√≠culo(s)")
                if len(articles) > 0:
                    print(
                        f"        Longitud promedio: {content_length / len(articles):.0f} caracteres"
                    )

        print()
        print("   " + "-" * 76)
        print()

        # Resumen
        print("4Ô∏è‚É£ Resumen de Calidad:")
        print()
        print(f"   ‚úÖ Fuentes Tier 1 (profundas): {tier1_count} art√≠culos")
        if tier1_count > 0:
            print(
                f"      ‚Üí Promedio de longitud: {tier1_total_chars / tier1_count:.0f} caracteres"
            )
            print(f"      ‚Üí Total de contenido: {tier1_total_chars:,} caracteres")

        print()
        print(f"   ‚úÖ Fuentes Tier 2 (APIs): {tier2_count} art√≠culos")
        if tier2_count > 0:
            print(
                f"      ‚Üí Promedio de longitud: {tier2_total_chars / tier2_count:.0f} caracteres"
            )

        print()
        print("   " + "-" * 76)
        print()

        # Verificaci√≥n de timeout
        print("5Ô∏è‚É£ Verificaci√≥n de Timeout:")
        print()

        if elapsed_time < 10:
            print("   ‚ö†Ô∏è ADVERTENCIA: La b√∫squeda fue muy r√°pida (<10s)")
            print("      Esto sugiere que solo se consultaron las APIs globales.")
            print("      Las fuentes locales probablemente fallaron o no se esperaron.")
        elif elapsed_time < 45:
            print("   ‚ö†Ô∏è ADVERTENCIA: La b√∫squeda fue r√°pida (<45s)")
            print("      Es posible que Infobae haya hecho timeout.")
            print("      Verifica que Camoufox est√© instalado: 'camoufox fetch'")
        elif elapsed_time >= 45 and tier1_count >= 6:
            print("   ‚úÖ PERFECTO: La b√∫squeda tom√≥ suficiente tiempo y obtuvo")
            print("      art√≠culos completos de fuentes locales.")
            print(f"      Tiempo: {elapsed_time:.1f}s es apropiado para obtener")
            print("      contenido de El Comercio, La Rep√∫blica e Infobae.")
        else:
            print(f"   ‚ÑπÔ∏è INFO: B√∫squeda completada en {elapsed_time:.1f}s")
            print(f"      Se obtuvieron {tier1_count} art√≠culos de fuentes locales.")

        print()

        # Verificaci√≥n de contenido completo
        print("6Ô∏è‚É£ Verificaci√≥n de Contenido Completo:")
        print()

        has_full_content = False
        for result in results:
            content = result.get("content", "")
            if len(content) > 1000 and result.get("source") in tier1_sources:
                has_full_content = True
                print(f"   ‚úÖ Art√≠culo completo detectado:")
                print(f"      Fuente: {result.get('source')}")
                print(f"      T√≠tulo: {result.get('title', 'N/A')[:60]}...")
                print(f"      Longitud: {len(content):,} caracteres")
                print(f"      Primeros 150 caracteres:")
                print(f'      "{content[:150]}..."')
                print()
                break

        if not has_full_content:
            print("   ‚ùå NO se detectaron art√≠culos completos (>1000 chars)")
            print("      Esto indica que las fuentes locales no completaron.")

        print()

    else:
        print("   ‚ùå ERROR: Respuesta sin campo 'results'")
        print(f"   Respuesta: {json.dumps(data, indent=2)}")

    print("=" * 80)
    print()

    # Conclusi√≥n
    if tier1_count >= 3 and has_full_content:
        print("üéâ CONCLUSI√ìN: El sistema est√° funcionando CORRECTAMENTE")
        print("   ‚Ä¢ El timeout de 90s es respetado")
        print("   ‚Ä¢ Las fuentes locales est√°n entregando contenido completo")
        print("   ‚Ä¢ El Investigator recibir√° informaci√≥n de calidad")
    elif tier1_count > 0 and has_full_content:
        print("‚ö†Ô∏è CONCLUSI√ìN: El sistema funciona PARCIALMENTE")
        print("   ‚Ä¢ Algunas fuentes locales est√°n funcionando")
        print(f"   ‚Ä¢ Solo {tier1_count} de 9 posibles fuentes locales respondieron")
        print("   ‚Ä¢ Verifica que todas las APIs locales est√©n configuradas")
    else:
        print("‚ùå CONCLUSI√ìN: El sistema NO est√° obteniendo contenido completo")
        print("   ‚Ä¢ Las fuentes locales no est√°n respondiendo")
        print("   ‚Ä¢ Posibles causas:")
        print("     1. ScraperRalf no tiene configuradas las fuentes locales")
        print("     2. Infobae requiere 'camoufox fetch'")
        print("     3. Timeout demasiado bajo en ScraperRalf")
        print("     4. Problemas de red/firewall")

    print()
    print("=" * 80)

except requests.Timeout:
    end_time = time.time()
    elapsed_time = end_time - start_time
    print()
    print(f"   ‚ùå TIMEOUT despu√©s de {elapsed_time:.2f} segundos")
    print()
    print("   üîç Diagn√≥stico:")
    print("      El timeout de 90s fue excedido. Esto puede indicar:")
    print("      1. ScraperRalf est√° tardando m√°s de 90s (muy poco probable)")
    print("      2. ScraperRalf se colg√≥ y no respondi√≥")
    print("      3. Problema de red/firewall")
    print()
    print("   üí° Soluciones:")
    print("      1. Revisa los logs de ScraperRalf: tail -f project.log")
    print("      2. Verifica que Infobae/Camoufox est√© funcionando")
    print("      3. Intenta con una query m√°s simple: 'econom√≠a'")

except Exception as e:
    print()
    print(f"   ‚ùå ERROR: {type(e).__name__}: {e}")
    print()
    print("   üí° Verifica que ScraperRalf est√© corriendo correctamente")

print()
